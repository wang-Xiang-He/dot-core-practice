{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_Tutorial_1(作業2解答).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d7fd5a02733249288f939c5ac092cb34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_649f41593d6a4c028388ebded2c1b5e5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ad452122602449ca81e7bd2024438ac1","IPY_MODEL_ca3e675a721b4e67aa11081d28767405"]}},"649f41593d6a4c028388ebded2c1b5e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad452122602449ca81e7bd2024438ac1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0de943532d62470a81067ec1c9a01306","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f521331a5fd84227a620cb2e1fe4f6a0"}},"ca3e675a721b4e67aa11081d28767405":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a609f40b4638400288fdb9215237ba90","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.98MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d409d49797d4a79b7f12edc956f5d2d"}},"0de943532d62470a81067ec1c9a01306":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f521331a5fd84227a620cb2e1fe4f6a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a609f40b4638400288fdb9215237ba90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0d409d49797d4a79b7f12edc956f5d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2664ec4632484406900ae9dc136485f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_aad337a0478347d088161749c75e6cfb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_168008b6513746e8bd2f7d1b7925bf3e","IPY_MODEL_edd3d2aa01234e128feffb2683c9171e"]}},"aad337a0478347d088161749c75e6cfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"168008b6513746e8bd2f7d1b7925bf3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e25ac513c88642c1a2d33f7873a7565a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea455109cc24430886d5892dffe39c5b"}},"edd3d2aa01234e128feffb2683c9171e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_04cf6221b1bc473c812bf63128e92bd2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:06&lt;00:00, 64.7B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_931d9286ff8f4fca8a50b900b8291c1f"}},"e25ac513c88642c1a2d33f7873a7565a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ea455109cc24430886d5892dffe39c5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"04cf6221b1bc473c812bf63128e92bd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"931d9286ff8f4fca8a50b900b8291c1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae7896957de446f68e25245c05ea8b46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_46de7a89d69e4067830f9683e00b0cf7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_efde7ecddfb54865a9737a0923a6f67e","IPY_MODEL_2a3cfeb6c7594d67865ebe951ca52ac2"]}},"46de7a89d69e4067830f9683e00b0cf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"efde7ecddfb54865a9737a0923a6f67e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7919a1c9cd3f4a27bb703a29ee2e1f39","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4acf1729d3cb49609d4041068f002bf9"}},"2a3cfeb6c7594d67865ebe951ca52ac2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3e36cdbc798e4d50b986b7f7cdf8c4b7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:06&lt;00:00, 69.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d7cd424a22b4e9baa135e72d0106b8a"}},"7919a1c9cd3f4a27bb703a29ee2e1f39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4acf1729d3cb49609d4041068f002bf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e36cdbc798e4d50b986b7f7cdf8c4b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1d7cd424a22b4e9baa135e72d0106b8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"IsIU8GXSUQDI"},"source":["# **Google BERT Transformers with Pytorch Example** "]},{"cell_type":"markdown","metadata":{"id":"NAUXCyYdUtnE"},"source":["**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**\n","\n","\n","PyTorch Transformers can be installed using pip below and few pre-requisites:\n","*   So change the Run Time type to GPU \n","*   Use version Python 3.5+ and PyTorch 1.1.0\n","*   Import the required Libraries\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VA_5qKN0iSVl"},"source":["## What is BERT?\n","A new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be fine- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- specific architecture modifications."]},{"cell_type":"markdown","metadata":{"id":"87HjTjkqAq_K"},"source":["There are a few things I want to explain in this section.\n","\n","1. It’s easy to get that BERT stands for Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one. For now, the key takeaway from this line is – BERT is based on the Transformer architecture.\n","\n","2. BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia(that’s 2,500 million words!) and Book Corpus (800 million words). This pretraining step is really important for BERT's success. This is because as we train a model on a large text corpus, our model starts to pick up the deeper and intimate understandings of how the language works. This knowledge is the swiss army knife that is useful for almost any NLP task.\n","\n","3. BERT is a deeply bidirectional model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase."]},{"cell_type":"markdown","metadata":{"id":"Y2cAxP2uAX5P"},"source":["BERT was built upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit — but crucially these models are all unidirectional or shallowly bidirectional. This means that each word is only contextualized using the words to its left (or right). \n","\n","\n","\n","For example, in the sentence I made a bank deposit the unidirectional representation of bank is only based on I made a but not deposit. Some previous work does combine the representations from separate left-context and right-context models, but only in a \"shallow\" manner.\n","\n"," **Example**- BERT represents \"bank\" using both its left and right context — \n"," **I made a ... deposit** — starting from the very bottom of a deep neural network, so it is deeply bidirectional."]},{"cell_type":"markdown","metadata":{"id":"-3IZV5jjBttx"},"source":["![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/sent_context.png)"]},{"cell_type":"markdown","metadata":{"id":"Mgo0da8mCGB2"},"source":["## Main concepts\n","\n","The library is build around three type of classes for each models:\n","\n","1. **model classes** which are PyTorch models (torch.nn.Modules) of the 8 models architectures currently provided in the library, e.g. BertModel\n","2. **configuration classes** which store all the parameters required to build a model, e.g. BertConfig. You don’t always need to instantiate these your-self, in particular if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)\n","3. **tokenizer classes** which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model, e.g. BertTokenizer\n","All these classes can be instantiated from pretrained instances and saved locally using two methods:\n","\n","**from_pretrained()** let you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself (currently 27 models are provided as listed here) or stored locally (or on a server) by the user.\n","\n","**save_pretrained()** let you save a model/configuration/tokenizer locally so that it can be reloaded using from_pretrained()."]},{"cell_type":"markdown","metadata":{"id":"3Go6_nxwchPB"},"source":["### 1. Bert Model Architecture\n","\n","BERT’s model architecture is a multi-layer bidirectional Transformer encoder\n","\n","1. **BERT-Large, Uncased (Whole Word Masking)**: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","2. **BERT-Large, Cased (Whole Word Masking)**: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","3. **BERT-Base, Uncased**: 12-layer, 768-hidden, 12-heads, 110M parameters\n","4. **BERT-Large, Uncased**: 24-layer, 1024-hidden, 16-heads, 340M parameters\n","5. **BERT-Base, Cased**: 12-layer, 768-hidden, 12-heads , 110M parameters\n","6. **BERT-Large, Cased**: 24-layer, 1024-hidden, 16-heads, 340M parameters"]},{"cell_type":"markdown","metadata":{"id":"Gs1mCcR-gWC0"},"source":["![](http://jalammar.github.io/images/bert-base-bert-large-encoders.png)"]},{"cell_type":"markdown","metadata":{"id":"HVA8QwkmkJAI"},"source":["We denote the number of layers (i.e., Transformer blocks) \n","as **L**, the **hidden size** as **H**, and \n","the number of self-attention heads as We primarily report results on \n","two model sizes: \n","1. **BERTBASE (L=12, H=768, A=12, Total Parameters=110M)** \n","2. **BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)**"]},{"cell_type":"markdown","metadata":{"id":"CXhYHZbhVh92"},"source":["Load pre-trained model tokenizer (vocabulary)\n","The base class PreTrainedTokenizer implements the common methods for loading/saving a tokenizer either from a local file or directory, or from a pretrained tokenizer provided by the library (downloaded from HuggingFace’s AWS S3 repository)."]},{"cell_type":"markdown","metadata":{"id":"7pPb2PwZFwZB"},"source":["```\n","class transformers.BertModel\n","```\n","Parameters\n","*   config (BertConfig) – Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model weights.\n","\n","**Example**\n","Indices of input sequence tokens in the vocabulary. To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n","\n","1. **For sequence pairs:**\n","\n","tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","\n","token_type_ids: 0   0  0    0    0    0    0   0   1  1  1  1 1   1\n","\n","2. **For single sequences:**\n","\n","tokens:[CLS] the dog is hairy . [SEP]\n","\n","token_type_ids:   0   0   0   0  0     0   0\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nqoLtjmRVzzV"},"source":["### 2. Bert Tokenizer\n","**BertTokenizer** = Tokenizer classes which store the vocabulary for each model and provide methods for encoding/decoding strings in list of token embeddings indices to be fed to a model eg DistilBertTokenizer,BertTokenizer etc\n","![](https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-1.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0VHjPc7rXoVy"},"source":["**PreTrainedTokenizer** is the main entry point into tokenizers as it also implements the main methods for using all the tokenizers:\n","\n","*   tokenizing, converting tokens to ids and back and encoding/decoding,\n","*   adding new tokens to the vocabulary in a way that is independant of the underlying structure (BPE, SentencePiece…),\n","*   managing special tokens (adding them, assigning them to roles, making sure they are not split during tokenization)\n","\n","**Main Class from where the Tokenizer is been called**\n","```\n","class transformers.BertTokenizer(vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, **kwargs)\n","\n","```\n","\n","*Parameters*\n","\n","\n","*  **vocab_file** – Path to a one-wordpiece-per-line vocabulary file\n","*  **do_lower_case** – Whether to lower case the input. Only has an effect when                     do_basic_tokenize=True\n","\n","*  **do_basic_tokenize** – Whether to do basic tokenization before wordpiece.\n","*  **max_len** – An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the minimum of this value (if             specified) and the underlying BERT model’s sequence length.\n","\n","*   **never_split** – List of tokens which will never be split during                        tokenization. Only has an effect when do_basic_tokenize=True\n"]},{"cell_type":"markdown","metadata":{"id":"_-Ji4oGwanPv"},"source":["![](https://www.researchgate.net/publication/323904682/figure/fig1/AS:606458626465792@1521602412057/The-Transformer-model-architecture.png)"]},{"cell_type":"markdown","metadata":{"id":"W8bBNBora1BJ"},"source":["So to have a detail architecture of how Encoder-Decoder works here is few [Link1](https://arxiv.org/pdf/1706.03762.pdf) & visual [Link2](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) "]},{"cell_type":"markdown","metadata":{"id":"FiXGe7Qlcy3v"},"source":["**ARCHITECTURE**: \n","1. **Encoder**: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\n","\n","2. **Decoder:** The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."]},{"cell_type":"markdown","metadata":{"id":"8RJLtIdAX5lX"},"source":["Let’s try to classify the sentence “a visually stunning rumination on love”. The first step is to use the BERT tokenizer to first split the word into tokens. Then, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence).\n","\n","[CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers)."]},{"cell_type":"markdown","metadata":{"id":"3Fc-rFzfEDim"},"source":["![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/bert_emnedding.png)"]},{"cell_type":"markdown","metadata":{"id":"0t0i3LnbFNjN"},"source":["BERT input representation. The **input embeddings** are the sum of the **token embeddings**, the **segmentation embeddings** and **the position embeddings**."]},{"cell_type":"markdown","metadata":{"id":"85eRapWMEigo"},"source":["1. The first token of every sequence is always a special clas- sification token (**[CLS]**). \n","2. The final hidden state corresponding to this token is used as the ag- gregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence.\n","3. We differentiate the sentences in two ways. First, we separate them with a special token (**[SEP]**). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. \n","4.A positional embedding is also added to each token to indicate its position in the sequence.\n"]},{"cell_type":"markdown","metadata":{"id":"mePP9GjSCpm9"},"source":["### 3. Configuration Class\n"]},{"cell_type":"markdown","metadata":{"id":"Gf1CkpfrGy0g"},"source":["The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace’s AWS S3 repository).\n","\n","\n","\n","```\n","class transformers.PretrainedConfig(**kwargs)\n","```\n","Base class for all configuration classes. Handles a few parameters common to all models’ configurations as well as methods for loading/downloading/saving configurations.\n","\n","*Parameters*\n","\n","*  **finetuning_task** – string, default None. Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\n","\n","*   **num_labels** – integer, default 2. Number of classes to use when the model is a classification model (sequences/tokens)\n","*   **output_hidden_states** – string, default False. Should the model returns all hidden-states.\n","\n","*   **output_attentions** – boolean, default False. Should the model returns attentions weights.\n","*   **torchscript** – string, default False. Is the model used with Torchscript.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WddVtaL9J92w"},"source":["# Model Building"]},{"cell_type":"markdown","metadata":{"id":"lZYdnHUpIklM"},"source":["Using BERT has two stages: Pre-training and fine-tuning.\n","\n","**Pre-training** :It is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a one-time procedure for each language (current models are English-only, but multilingual models will be released in the near future). We are releasing a number of pre-trained models from the paper which were pre-trained at Google. Most NLP researchers will never need to pre-train their own model from scratch.\n","\n","**Fine-tuning** : It is inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. SQuAD, for example, can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%, which is the single system state-of-the-art.\n","\n","\n","![](https://insidebigdata.com/wp-content/uploads/2019/10/Peltarion_pic2.jpg)"]},{"cell_type":"markdown","metadata":{"id":"iITsqEiLJJGd"},"source":["Here we will see two pre-trained Models\n","> *Model 1*: **Masked LM**\n","\n","> *Model 2*: **Next Sentence Prediction (NSP)**"]},{"cell_type":"markdown","metadata":{"id":"khJSahp4bReO"},"source":["## Model 1:Masked LM"]},{"cell_type":"markdown","metadata":{"id":"LiV-JiwbbXE-"},"source":["**STEPS**\n","\n","1. Deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. \n"," the model could trivially predict the target word in a multi-layered context.\n","\n","2. In order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. \n","\n","3. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. \n","\n","\n","Although this allows us to obtain a bidirectional pretrained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the **[MASK]** token does not appear during fine-tuning. To mitigate this, we do not always replace “masked” words with the actual **[MASK]** token. \n","\n","**1 Problem:** Language models only use left context or right context, but language understanding is bidirectional.\n","\n","● Why are LMs unidirectional?\n","\n","*Reason 1:* Directionality is needed to generate a\n","well-formed probability distribution.\n","\n","    ○ We don’t care about this.\n","● \n","*Reason 2:* Words can “see themselves” in a bidirectional encoder.\n","\n","**Solution:** Mask out k% of the input words, \n","and then predict the masked words\n","○ We always use k = 15%\n","\n","\n","                                store                 gallon\n","                                  |                    |\n","          the man went to the **[MASK]**  to buy a **[MASK]**  of milk\n","\n","● Too little masking: Too expensive to train\n","\n","● Too much masking: Not enough context\n","\n","\n","\n","**2 Problem:** Mask token never seen at fine-tuning\n","\n","**Solution:** 15% of the words to predict, but don’t\n","replace with **[MASK]** 100% of the time. Instead:\n","\n","● 80% of the time, replace with **[MASK]** \n","went to the store → went to the **[MASK]** \n","\n","● 10% of the time, replace random word\n","went to the store → went to the running\n","\n","● 10% of the time, keep same\n","went to the store → went to the store\n","\n","\n","If the i-th token is chosen, we replace the i-th token with Then, Ti will be used to predict the original token with cross entropy loss.\n","\n"]},{"cell_type":"code","metadata":{"id":"AInZF3z5IOru","executionInfo":{"status":"ok","timestamp":1603956451699,"user_tz":-480,"elapsed":12567,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"253fc9cc-fe23-48e1-82b1-5bd7933e528e","colab":{"base_uri":"https://localhost:8080/"}},"source":["#載入 Transformer\n","!pip install transformers\n","# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","import torch\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n","\r\u001b[K     |▎                               | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 25.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 17.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 10.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 9.0MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 256kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 266kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 276kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 286kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 296kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 307kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 317kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 327kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 337kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 348kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 358kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 368kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 378kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 389kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 399kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 409kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 419kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 430kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 440kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 450kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 460kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 471kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 481kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 491kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 501kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 512kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 522kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 532kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 542kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 552kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 563kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 573kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 583kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 593kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 604kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 614kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 624kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 634kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 645kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 655kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 665kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 675kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 686kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 696kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 706kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 716kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 727kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 737kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 747kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 757kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 768kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 778kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 788kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 798kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 808kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 819kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 829kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 839kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 849kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 860kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 870kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 880kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 890kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 901kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 911kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 921kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 931kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 942kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 952kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 962kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 972kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 983kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 993kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.0MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 8.8MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.9.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 46.4MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 48.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 48.5MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7e1539d5d80dde9f07326ccb7e7b2a1024b92139e1225e0265287c5232e96b7e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Azh1mYBfcZKE","executionInfo":{"status":"ok","timestamp":1603956459943,"user_tz":-480,"elapsed":703,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"3bfa5049-0d7c-47a3-d24b-ae4cfb68231f","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["d7fd5a02733249288f939c5ac092cb34","649f41593d6a4c028388ebded2c1b5e5","ad452122602449ca81e7bd2024438ac1","ca3e675a721b4e67aa11081d28767405","0de943532d62470a81067ec1c9a01306","f521331a5fd84227a620cb2e1fe4f6a0","a609f40b4638400288fdb9215237ba90","0d409d49797d4a79b7f12edc956f5d2d"]}},"source":["# Load pre-trained model tokenizer (vocabulary)\n","# 載入pretrained bert-base-uncased tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:filelock:Lock 140095527062384 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7fd5a02733249288f939c5ac092cb34","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 140095527062384 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sdEqm_R3ccHX","executionInfo":{"status":"ok","timestamp":1603956861105,"user_tz":-480,"elapsed":793,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"5fb2fbc0-f376-4176-e6f6-0152b316ac66","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Tokenize input\n","# 測試例句[CLS].....[SEP]...[SEP]\n","text  = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n","tokenized_text = tokenizer.tokenize(text)\n","#轉置矩陣\n","#tokenized_text.T\n","tokenized_text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'who',\n"," 'was',\n"," 'jim',\n"," 'henson',\n"," '?',\n"," '[SEP]',\n"," 'jim',\n"," 'henson',\n"," 'was',\n"," 'a',\n"," 'puppet',\n"," '##eer',\n"," '[SEP]']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"nOB1PfvSfZbM"},"source":["Here MASK is a word we will predict between the sentence\n","\n","\n","**TOKEN EMBEDDINGS**"]},{"cell_type":"code","metadata":{"id":"6IGr3YHRchsk"},"source":["# Mask a token that we will try to predict back with `BertForMaskedLM` \n","# 將 'hanson' 以 [MASK]取代\n","masked_index = 8\n","tokenized_text[masked_index] = '[MASK]'\n","assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQME6Hyzc9bN","executionInfo":{"status":"ok","timestamp":1603956905340,"user_tz":-480,"elapsed":939,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"9363d86a-b641-4cd9-f927-e201d40fd466","colab":{"base_uri":"https://localhost:8080/"}},"source":["tokenized_text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'who',\n"," 'was',\n"," 'jim',\n"," 'henson',\n"," '?',\n"," '[SEP]',\n"," 'jim',\n"," '[MASK]',\n"," 'was',\n"," 'a',\n"," 'puppet',\n"," '##eer',\n"," '[SEP]']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"sYFFDHAXhDE3"},"source":["**POSITION EMBEDDINGS**"]},{"cell_type":"code","metadata":{"id":"prohNWKVcjfx","executionInfo":{"status":"ok","timestamp":1603956952805,"user_tz":-480,"elapsed":986,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"c1dc6c28-ffbd-425e-df05-65d2a4af3848","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Convert token to vocabulary indices\n","# 將字符轉成索引 index\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","indexed_tokens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101,\n"," 2040,\n"," 2001,\n"," 3958,\n"," 27227,\n"," 1029,\n"," 102,\n"," 3958,\n"," 103,\n"," 2001,\n"," 1037,\n"," 13997,\n"," 11510,\n"," 102]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"E5J4NYdQhln7"},"source":["**SEGMENT EMBEDDINGS**\n","\n","Here from the tokenized tokens which are part of one sentence we indexing with a 0,1 respectively for each sentence."]},{"cell_type":"code","metadata":{"id":"X8WapubcclMV"},"source":["# 指出屬於哪個句段\n","# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"djEHZYr7cm1G"},"source":["# Convert inputs to PyTorch tensors\n","# position embeddings\n","tokens_tensor = torch.tensor([indexed_tokens])\n","#segment embeddings\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09_n9_ifdZHS","executionInfo":{"status":"ok","timestamp":1603956999854,"user_tz":-480,"elapsed":9503,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"41047497-cd58-4534-c3d5-adda3fecf496","colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["2664ec4632484406900ae9dc136485f6","aad337a0478347d088161749c75e6cfb","168008b6513746e8bd2f7d1b7925bf3e","edd3d2aa01234e128feffb2683c9171e","e25ac513c88642c1a2d33f7873a7565a","ea455109cc24430886d5892dffe39c5b","04cf6221b1bc473c812bf63128e92bd2","931d9286ff8f4fca8a50b900b8291c1f","ae7896957de446f68e25245c05ea8b46","46de7a89d69e4067830f9683e00b0cf7","efde7ecddfb54865a9737a0923a6f67e","2a3cfeb6c7594d67865ebe951ca52ac2","7919a1c9cd3f4a27bb703a29ee2e1f39","4acf1729d3cb49609d4041068f002bf9","3e36cdbc798e4d50b986b7f7cdf8c4b7","1d7cd424a22b4e9baa135e72d0106b8a"]}},"source":["# Load pre-trained model (weights)\n","# 載入已訓練好的model weights 建模\n","model = BertModel.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:filelock:Lock 140093124242624 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2664ec4632484406900ae9dc136485f6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 140093124242624 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n","INFO:filelock:Lock 140093124240384 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae7896957de446f68e25245c05ea8b46","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:filelock:Lock 140093124240384 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fzLgAl3Ilti8"},"source":["**Hyperparameter Tuning**\n","  1. layer_norm_eps: 1e-12\n","  2. max_position_embeddings: 512\n","  3. num_attention_heads: 12\n","  4. num_hidden_layers: 12\n","  5. num_labels: 2\n","  10. torchscript: false\n","  11. type_vocab_size: 2\n","  13. vocab_size: 30522"]},{"cell_type":"code","metadata":{"id":"_vR9k4P5disP","executionInfo":{"status":"ok","timestamp":1603957011221,"user_tz":-480,"elapsed":671,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"a73b6a4d-94ac-4311-8861-b76ae4dee365","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Set the model in evaluation mode to deactivate the DropOut modules\n","# This is IMPORTANT to have reproducible results during evaluation!\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"0BoPAjibdlkt","executionInfo":{"status":"ok","timestamp":1603957032683,"user_tz":-480,"elapsed":14396,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"b3acbc5a-4dd8-47bb-bed5-240ab7efd260","colab":{"base_uri":"https://localhost:8080/"}},"source":["# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"DcIqReM6d_nQ"},"source":["\n","# Predict hidden states features for each layer\n","with torch.no_grad():\n","    # See the models docstrings for the detail of the inputs\n","    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    # Transformers models always output tuples.\n","    # See the models docstrings for the detail of all the outputs\n","    # In our case, the first element is the hidden state of the last layer of the Bert model\n","    #BERT最後層的隱藏層第一個元素[CLS]就是我們的輸入\n","    encoded_layers = outputs[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BLZ8_uifLaX","executionInfo":{"status":"ok","timestamp":1603957061011,"user_tz":-480,"elapsed":703,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"f28bae7e-47fe-4934-9edf-dfdcd2ab60b5","colab":{"base_uri":"https://localhost:8080/"}},"source":["#1句，14 tokens, 768隱藏元\n","encoded_layers.shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 14, 768])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"eW66dStfj-_V"},"source":[" So from the above shape we have 3D tensor.\n"," **SHAPE(BATCH SIZE,TOKENS,HIDDEN UNITS)**\n","\n",">  **1** represents one sentence\n","\n",">  **14** represents number of total tokens after tokenization\n","\n",">  **768** the number of hidden units in the **Bert model Uncased**.\n","\n"]},{"cell_type":"code","metadata":{"id":"gofsKTVpj1wE","executionInfo":{"status":"ok","timestamp":1603957090871,"user_tz":-480,"elapsed":973,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"b14c35b5-ee26-4f6a-abd5-a8da0a6502de","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(len(indexed_tokens), model.config.hidden_size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["14 768\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"362pUiQheC2U"},"source":["# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n","# 確認shape正確\n","assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yemSMWgDfbtY","executionInfo":{"status":"ok","timestamp":1603957118312,"user_tz":-480,"elapsed":5001,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"c645b8e4-3c70-4a2c-efe1-4e15a63b5c66","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"6lFBXOWvfs1X","executionInfo":{"status":"ok","timestamp":1603957125043,"user_tz":-480,"elapsed":759,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"4d7a0d05-b1a0-4f7d-cd79-8f5f23e3d216","colab":{"base_uri":"https://localhost:8080/"}},"source":["# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"WsIoJsv5gOZ3"},"source":["# Predict all tokens\n","with torch.no_grad():\n","    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    predictions = outputs[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbvpC5tOgVIX","executionInfo":{"status":"ok","timestamp":1603957140111,"user_tz":-480,"elapsed":660,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"a10986aa-1810-47fb-a89b-5ca443fbf700","colab":{"base_uri":"https://localhost:8080/"}},"source":["#執行預測\n","predictions"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ -7.8798,  -7.7874,  -7.7861,  ...,  -7.0438,  -6.7454,  -4.6013],\n","         [-13.3633, -13.7694, -13.7819,  ..., -11.8128, -11.1635, -13.8906],\n","         [-10.9775, -10.5383, -10.9659,  ..., -11.5549,  -8.0309,  -6.3979],\n","         ...,\n","         [ -5.2284,  -5.6572,  -5.3550,  ...,  -3.4507,  -3.8718,  -8.6904],\n","         [ -8.5290,  -8.4146,  -9.0744,  ...,  -7.1710,  -6.9877,  -6.1301],\n","         [-12.5968, -12.3769, -12.4222,  ..., -10.1020,  -9.8764,  -9.4495]]],\n","       device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"-fEdAPEAgRYV","executionInfo":{"status":"ok","timestamp":1603957147357,"user_tz":-480,"elapsed":717,"user":{"displayName":"Paul Chen","photoUrl":"","userId":"15113937160468943492"}},"outputId":"7d003184-3624-4be5-c202-22c1514e19ee","colab":{"base_uri":"https://localhost:8080/"}},"source":["# confirm we were able to predict 'henson'\n","# 用argmax找出機會最大的index\n","# 再用index把字找出來\n","predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","print(predicted_index,predicted_token)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["27227 henson\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7TkRUsuhgkb9"},"source":["assert predicted_token == 'henson'"],"execution_count":null,"outputs":[]}]}